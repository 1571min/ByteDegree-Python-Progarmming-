{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim으로 네이버 기사 토픽 모델링 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 토픽 모델링을 적용하기 위해 텍스트를 처리합니다.\n",
    "\n",
    "> 토픽 모델링 라이브러리인 gensim을 사용해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 토픽 모델링을 위한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar\n",
    "# Mecab, Okt 등 형태소 분석기 불러오기\n",
    "# 특수문자\n",
    "# 경고 알림 제거를 위한 라이브러리\n",
    "# gensim에서 사용하는 vectorizer 모듈과, LDA model을 불러온다.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # 경고 알림이 뜨면 모두 무시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 전처리 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(input_file_name):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # pk 파일을 읽어서 리스트로 변환하여 돌려줌.\n",
    "\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 함수를 편의를 위해 조금 변형해보자.\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "        \n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # 특수 문자를 추가해보자.\n",
    "\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "\n",
    "    return SW\n",
    "\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    # 명사 추출 / 형태소 분석 두 가지를 선택할 수 있게 만들어주는 함수를 만들어보자.\n",
    " \n",
    "    # tqdm을 사용하여 진행 과정을 볼 수 있게 만들어보자.\n",
    "\n",
    "    return token_corpus\n",
    "\n",
    "# 함수를 불러오는 (메인) 코드.\n",
    "input_file_name = \"data/naver_news_content.pk\"\n",
    "documents = read_documents(input_file_name)\n",
    "SW = define_stopwords(\"data/stopwords-ko.txt\")\n",
    "cleaned_text = text_cleaning(documents)\n",
    "tokenized_text = text_tokenizing(cleaned_text, tokenizer=\"noun\") #tokenizer= \"noun\" or \"morph\" or \"word\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 읽기의 과정은 앞서 단어 임베딩의 경우와 다르지 않다. 다음 과정은 문서-단어 행렬을 만드는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 토픽 모델링에 사용할 함수들 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서-단어 행렬 만들기\n",
    "# 어휘(vocabulary) 학습\n",
    "\n",
    "# 문서-단어 행렬(document-term matrix) 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF 문서-단어 행렬 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA 결과 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 토픽 모델링을 추가하여 코드 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 토픽 개수, 키워드 개수를 정해주는 변수를 추가.\n",
    "\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    # 문서-단어 행렬 만들어주는 함수.\n",
    "    print(\"Building document-term matrix.\")\n",
    "\n",
    "\n",
    "\n",
    "def print_topic_words(model):\n",
    "\n",
    "    # 토픽 모델링 결과를 출력해 주는 함수.\n",
    "    print(\"\\nPrinting topic words.\\n\")\n",
    "\n",
    "\n",
    "# document-term matrix를 만들고,\n",
    "\n",
    "# LDA를 실행.\n",
    "\n",
    "# 결과를 출력.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. pyLDAvis를 통한 토픽 모델링 결과 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis 불러오기\n",
    "\n",
    "# pyLDAvis를 jupyter notebook에서 실행할 수 있게 활성화.\n",
    "\n",
    "# pyLDAvis 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트분석기법",
   "courseyear": "2018",
   "date": "2018.04.18",
   "logofile": "figs/ewhauniv-logo.png",
   "logoraise": "-.2",
   "logoscale": ".4",
   "title": "단어 임베딩과 토픽 모델링"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
